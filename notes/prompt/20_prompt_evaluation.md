# Prompt 教程 20：怎么评估一个 Prompt 是否“好用”

Prompt 好不好，不靠感觉，最好靠**可重复的评估**。

## 1. 评估维度（建议最少 4 个）
1) **正确性**：是否回答了问题？是否存在明显错误？
2) **稳定性**：同类输入是否输出结构一致？
3) **可控性**：是否遵守格式、条数、长度、语言等约束？
4) **可追溯性**（配合 RAG）：是否能指出依据？是否有引用？

## 2. 建一个最小评测集
你可以像 `demos/rag_min/eval/questions.jsonl` 那样，维护一个小集合：
- 10~30 条“常问问题”
- 每条给期望命中文档（expected_sources）
- 可选：给关键词（expected_keywords）

评测不是追求一次到位，而是：
- 先能衡量趋势（改 Prompt 后指标变好/变坏）
- 发现最常见的失败模式，再迭代

## 3. 常用的验收（Rubric）写法
在 Prompt 末尾追加 Rubric，例如：

- 必须输出 5 条要点
- 每条要点必须附引用
- 如果检索材料不足，必须说明“缺少哪些资料”

Rubric 的作用是把“主观好坏”变成“可判定合格/不合格”。

## 4. 与工程化的关系
当 Prompt、RAG、Tool 组合成 Agent 后，评估就更重要：
- Prompt 变更会影响输出结构
- chunking 或索引变更会影响命中率
- 工具返回结构变更会影响后续链路

所以建议你把“评测集”当成仓库的一部分长期维护。

---

## 📌 导航
- 上一章：[Prompt 教程 10：常用 Prompt 模式](./10_prompt_patterns.md)
- 下一章：建议进入 [RAG 教程 00：RAG 是什么](../rag/00_overview.md)
- 返回总览：[根目录学习导航](../../README.md#-专题学习导航prompt--rag--skill)
