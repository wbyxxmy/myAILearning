# RAG 教程 40：怎么评测 RAG？从“能跑”到“能用”的关键一步

RAG 做到“能跑通”只是开始；做到“能用”，就必须能回答两个问题：

1) **检索到底好不好？**
2) **改动（文档/切块/模型/索引）后是变好还是变坏？**

这篇教程给你一套从轻量到工程化的评测方法，优先适配你当前仓库的离线 Demo（模板式回答、不接大模型）。

---

## 1. 先明确：RAG 里要评什么
RAG 典型链路分三段：

- **数据层**：文档内容是否正确、是否完整、是否最新
- **检索层**：chunking + embedding + 索引 + 查询策略
- **生成层**：如何基于证据组织答案（你当前用模板式回答替代）

在你目前的阶段，建议把精力放在**检索层评测**，因为它最可量化、也最影响后续上限。

---

## 2. 最小评测集怎么建（建议从 20 条开始）
你可以像 `demos/rag_min/eval/questions.jsonl` 那样维护评测集。每条样例建议包含：

- `question`：用户会真实问的问题（不要只写教科书式问题）
- `expected_sources`：期望命中的文档文件名（最小就够）
- （可选）`expected_chunks`：期望命中 chunk 的 id（更严格，但维护成本更高）
- （可选）`expected_keywords`：答案中应覆盖的关键词（用于生成评测）

### 2.1 评测集从哪里来
- 从你的笔记目录标题反推（每个标题至少 1 个问题）
- 从自己复习时的真实疑问记录下来
- 从“失败案例”里累积（最有效）

### 2.2 样例设计建议
把问题分三类，各占 1/3：
- **定义类**：什么是 RAG / Chunking / Tool schema？
- **流程类**：RAG 的步骤是什么？索引如何构建？
- **对比/选择类**：什么时候用混合检索？TopK 怎么选？

---

## 3. 检索层最常用的 4 个指标（够用且好实现）

### 3.1 Recall@K（命中率）
含义：TopK 检索结果里是否出现了“期望来源”。

- 适合早期：你只需要关心“有没有检索到正确文档”
- 你 demo 里的 `run_eval.py` 就是这个思路

> 经验：早期把 Recall@5 做到 0.8+（80%）通常就有继续做下去的价值。

### 3.2 MRR（Mean Reciprocal Rank）
含义：第一个正确结果出现得越靠前越好。

- 如果正确来源常常排在第 5 名，那用户体验会差
- MRR 能体现“排序质量”

计算方式（直观理解）：
- 正确结果排第 1 → 得分 1.0
- 排第 2 → 0.5
- 排第 5 → 0.2
- TopK 都没命中 → 0

### 3.3 Precision@K（前 K 里“有用”的比例）
含义：TopK 里有多少条是“相关的”。

早期不太好标注“相关/不相关”，但当你规模变大，这个指标会越来越重要，因为 TopK 噪声多会拖累后续生成。

### 3.4 覆盖率（Coverage）
含义：你的评测集问题是否覆盖了知识库的主要主题？

做法很简单：
- 统计 `expected_sources` 的分布
- 看是否某些核心文档从来没被测到

---

## 4. 如何做“失败案例分析”（这是提升最快的部分）
评测的意义不是分数本身，而是帮你定位瓶颈。常见失败原因对照表：

### 4.1 没命中正确来源（Recall 失败）
可能原因：
- 文档里根本没有答案（数据缺失）
- chunking 把定义切碎了（chunk 太小/太散）
- embedding 模型对中文效果一般（或领域词汇不适配）
- 问题表达与文档表述差异大（需要 query rewrite / 同义词）

排查建议：
1) 手工搜索文档：答案到底在不在
2) 打印 topK 命中片段：是不是“看起来很像但不对”
3) 调 chunking：按标题切、合并短段、保留列表结构
4) 调 K：K 太小可能漏，先临时调大做定位

### 4.2 命中了但排得很靠后（MRR 低）
可能原因：
- 相似度度量不足（需要更强 embedding 或 rerank）
- chunk 太长导致噪声大
- 正确 chunk 含关键信息很少，被“更像问题”的段落压下去

排查建议：
- 观察正确 chunk 与前面错误 chunk 的差异
- 为关键概念在文档中加入更明确的小标题/关键词（文档本身也可以为检索服务）

---

## 5. 迭代策略：一次只改一个变量
为了知道“为什么变好/变坏”，建议一次只改一个因素，例如：

- 只改 chunk 大小（target_chars / overlap）
- 只换 embedding 模型
- 只改检索 K
- 只改 query（例如加入简化/改写）

每次改动后跑一遍 eval，把结果记录下来（建议写到 `eval/results/` 或 README 里）。

---

## 6. 进阶：当你接入大模型生成后，还要评什么
你现在是“模板式回答”。将来接 LLM 后，需要额外评测：

- **引用一致性**：答案中的每条结论是否能在引用片段中找到依据？
- **忠实性（Faithfulness）**：模型有没有“超出证据”做推断或编造？
- **可读性与结构**：是否按格式输出、是否简洁

如果你未来要做“强制引用”，可以把“引用覆盖率”变成硬指标：没有引用的句子算不合格。

---

## 7. 给你一个可落地的最小行动清单
1) 先把评测集扩到 20 条（覆盖定义/流程/对比）
2) 跑 `Recall@5` 与 `MRR@5`
3) 针对失败样本调 chunking（优先）
4) 再考虑换 embedding 或引入 rerank（后续）

做到这一步，你的 RAG 就从“玩具 Demo”进入“可控系统”了。

---

## 📌 导航
- 上一章：[RAG 教程 30：引用与 Grounding](./30_citations_and_grounding.md)
- 下一章：建议进入 [Skill/Tool 教程 00：为什么 Agent 需要工具](../skills/00_overview.md)
- 返回总览：[根目录学习导航](../../README.md#-专题学习导航prompt--rag--skill)
